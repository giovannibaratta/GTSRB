{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ModelBuildStep1TPU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giovannibaratta/GTSRB/blob/master/ModelBuildStep1TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfHIvRgmF2bK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Import e definizioni { vertical-output: true, display-mode: \"form\" }\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import os\n",
        "import time\n",
        "import pprint\n",
        "import math\n",
        "import random as rn\n",
        "import pickle\n",
        "#data aug\n",
        "from tensorflow import keras \n",
        "# gestione directory directory\n",
        "import shutil\n",
        "import sys\n",
        "# trace di errori\n",
        "import traceback\n",
        "# colorare output\n",
        "!pip install colorama\n",
        "from colorama import Fore, Style\n",
        "\n",
        "rootPath = '/gdrive/My Drive/DatasetSegnaliStradali/GTSRB/' #@param{type:'string'}\n",
        "\n",
        "useTPU = True\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "if useTPU == False:\n",
        "  print(tf.test.gpu_device_name())\n",
        "else:\n",
        "  if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "    print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "  else:\n",
        "    tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    print ('TPU address is', tpu_address)\n",
        "\n",
        "    #with tf.Session(tpu_address) as session:\n",
        "    #  devices = session.list_devices()\n",
        "\n",
        "    #print('TPU devices:')\n",
        "    #pprint.pprint(devices)\n",
        "      \n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "#import script da gdrive\n",
        "scriptPath = rootPath + \"scripts/colab/\"\n",
        "sys.path.append(scriptPath)\n",
        "from CommonUtils import *\n",
        "from ModelBuilderUtils import *\n",
        "from TrainingUtils import *\n",
        "\n",
        "resetSeed()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1CDs1W0GIft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Caricamento dei dati { vertical-output: true, display-mode: \"form\" }\n",
        "\n",
        "#riproducibilità dei training\n",
        "resetSeed()\n",
        "    \n",
        "trainingDir = rootPath + 'data/training/'\n",
        "validationDir = rootPath + 'data/validation/'\n",
        "\n",
        "width = 48#@param{type:'integer'}\n",
        "height = 48#@param{type:'integer'}\n",
        "\n",
        "useCustomLoad = False #@param {type:\"boolean\"}\n",
        "balanceClass = False #@param {type:\"boolean\"}\n",
        "customRatio = { \n",
        "                  0 : 1.3,\n",
        "                  1 : 0.5,\n",
        "                  2 : 0.5,\n",
        "                  3 : 0.7,\n",
        "                  4 : 0.5,\n",
        "                  5 : 0.5,\n",
        "                  6 : 1,\n",
        "                  7 : 0.7,\n",
        "                  8 : 0.7,\n",
        "                  9 : 0.7,\n",
        "                  10 : 0.5,\n",
        "                  11 : 0.7,\n",
        "                  12 : 0.5,\n",
        "                  13 : 0.5,\n",
        "                  14 : 1,\n",
        "                  15 : 1,\n",
        "                  16 : 1,\n",
        "                  17 : 1,\n",
        "                  18 : 0.7,\n",
        "                  19 : 1.3,\n",
        "                  20 : 1,\n",
        "                  21 : 1,\n",
        "                  22 : 1,\n",
        "                  23 : 1,\n",
        "                  24 : 1.2,\n",
        "                  25 : 0.5,\n",
        "                  26 : 0.7,\n",
        "                  27 : 1.2,\n",
        "                  28 : 1,\n",
        "                  29 : 1,\n",
        "                  30 : 1,\n",
        "                  31 : 1,\n",
        "                  32 : 1.2,\n",
        "                  33 : 1,\n",
        "                  34 : 1,\n",
        "                  35 : 0.7,\n",
        "                  36 : 1,\n",
        "                  37 : 1.2,\n",
        "                  38 : 0.5,\n",
        "                  39 : 1,\n",
        "                  40 : 1,\n",
        "                  41 : 1,\n",
        "                  42 : 1\n",
        "              }\n",
        "\n",
        "useDataAugmentation = False #@param {type:\"boolean\"}\n",
        "NUMBER_OF_TRAINING_AUG = 3 #@param {type:\"slider\", min:0, max:20, step:1}\n",
        "NUMBER_OF_VALIDATION_AUG = 1 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "\n",
        "generatorCustomLoad = keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=5,\n",
        "    width_shift_range=3,\n",
        "    height_shift_range=3,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=(0.95,1.05),\n",
        "    channel_shift_range=0.01)\n",
        "\n",
        "generatorTraining = keras.preprocessing.image.ImageDataGenerator(\n",
        "    brightness_range = (0.25,0.6),\n",
        "    rotation_range=25,\n",
        "    width_shift_range=8,\n",
        "    height_shift_range=8,\n",
        "    shear_range=15,\n",
        "    zoom_range=(0.4,1.15),\n",
        "    channel_shift_range=0.13)\n",
        "\n",
        "generatorValidation = keras.preprocessing.image.ImageDataGenerator(\n",
        "    brightness_range = (0.25,0.7),\n",
        "    rotation_range=25,\n",
        "    width_shift_range=8,\n",
        "    height_shift_range=8,\n",
        "    shear_range=0.4,\n",
        "    zoom_range=(0.4,1.15),\n",
        "    channel_shift_range=0.12)\n",
        "\n",
        "if useCustomLoad == False:\n",
        "  for key in customRatio.keys():\n",
        "    customRatio[key] = 1.0\n",
        "    \n",
        "if useCustomLoad == True and balanceClass == True:\n",
        "  trClassCount = {}\n",
        "  files = list(filter(lambda fn : fn.startswith('resized'), os.listdir(trainingDir)))\n",
        "  for fileName in files:\n",
        "    classLabel = int(fileName.replace(\"resized\",\"\"))\n",
        "    numFilePath = trainingDir + 'num' + str(classLabel)\n",
        "    with open(numFilePath, 'r') as file:\n",
        "      originalNum = int(file.readline())\n",
        "      trClassCount[classLabel] = originalNum\n",
        "\n",
        "  newMin = min(trClassCount.values()) * 1.3\n",
        "  for key in trClassCount.keys():\n",
        "    val =  trClassCount[key]\n",
        "    if val < newMin:\n",
        "      customRatio[key] = max(newMin/val, 1.3)\n",
        "    elif val == newMin:\n",
        "      customRatio[key] = 1.0\n",
        "    elif val > newMin:\n",
        "      if val > newMin/2:\n",
        "        customRatio[key] = max(newMin/val, 0.4)\n",
        "      else:\n",
        "        customRatio[key] = max(newMin/val, 0.5)\n",
        "\n",
        "# numero di immagini originali disponibili per ogni classe\n",
        "trainingClassCount = {}\n",
        "validationClassCount = {}\n",
        "totalTrainingCount = 0\n",
        "\n",
        "#leggo quante img ci sono per ogni class di training \n",
        "files = list(filter(lambda fn : fn.startswith('resized'), os.listdir(trainingDir)))\n",
        "for fileName in files:\n",
        "  classLabel = int(fileName.replace(\"resized\",\"\"))\n",
        "  numFilePath = trainingDir + 'num' + str(classLabel)\n",
        "  with open(numFilePath, 'r') as file:\n",
        "    originalNum = int(file.readline())\n",
        "    trainingClassCount[classLabel] = originalNum\n",
        "    totalTrainingCount += round(originalNum * customRatio[classLabel])\n",
        "\n",
        "if useDataAugmentation == True:\n",
        "  originaltotalTrainingCount = totalTrainingCount\n",
        "  totalTrainingCount = totalTrainingCount * (NUMBER_OF_TRAINING_AUG + 1)\n",
        "    \n",
        "trainingPadEnd = False #inserire altre immagini alla fine per essere divisibile per 8\n",
        "\n",
        "print(\"Training images (no pad):\", totalTrainingCount)\n",
        "\n",
        "if totalTrainingCount % 8 != 0:\n",
        "  # se uso le TPU la lunghezza dei dati deve essere divisibile per 8\n",
        "  trainingToPad = 8 - (totalTrainingCount % 8)\n",
        "  totalTrainingCount = totalTrainingCount + (8 - (totalTrainingCount % 8))\n",
        "  trainingPadEnd = True\n",
        "  \n",
        "#preparo gli array che contengono le img di training\n",
        "trainingData = np.empty((totalTrainingCount, width, height, 3), dtype='float32')\n",
        "trainingLabels = np.empty((totalTrainingCount), dtype='uint8')\n",
        "\n",
        "#leggo quante img ci sono per ogni class di validation\n",
        "validationNumber = 0\n",
        "\n",
        "for fileName in files:\n",
        "  classLabel = int(fileName.replace(\"resized\",\"\"))\n",
        "  numFilePath = validationDir + 'num' + str(classLabel)\n",
        "  with open(numFilePath, 'r') as file:\n",
        "    numFile = int(file.readline())\n",
        "    validationClassCount[classLabel] = numFile\n",
        "    validationNumber += numFile\n",
        "\n",
        "validationPadEnd = False #inserire altre immagini alla fine per essere divisibile per 8\n",
        "\n",
        "if useDataAugmentation == True:\n",
        "  originalValidationNumber = validationNumber\n",
        "  validationNumber = validationNumber * (NUMBER_OF_VALIDATION_AUG + 1)\n",
        "\n",
        "print(\"Validation images (no pad):\", validationNumber)\n",
        "\n",
        "if validationNumber % 8 != 0:\n",
        "  # se uso le TPU la lunghezza dei dati deve essere divisibile per 8\n",
        "  validationToPad = 8 - (validationNumber % 8)\n",
        "  validationNumber = validationNumber + (8 - (validationNumber % 8))\n",
        "  validationPadEnd = True\n",
        "  \n",
        "#preparo gli array che contengono le img di validation\n",
        "validationData = np.empty((validationNumber, width, height, 3), dtype='float32')\n",
        "validationLabels = np.empty((validationNumber), dtype='uint8')\n",
        "\n",
        "print('Total', validationNumber + totalTrainingCount)\n",
        "if useDataAugmentation == True:\n",
        "  print(\"Data augmentation training\", NUMBER_OF_TRAINING_AUG, \"validation\", NUMBER_OF_VALIDATION_AUG)\n",
        "else:\n",
        "  print(\"Data augmentation : OFF\")\n",
        "  \n",
        "validationCount = 0\n",
        "trainingCount = 0\n",
        "loadedClass = 0\n",
        "\n",
        "#caricamento dati\n",
        "for fileName in files:\n",
        "  classLabel = int(fileName.replace(\"resized\",\"\"))\n",
        "  #carico le img di training originali\n",
        "  trainingImages = np.memmap(trainingDir + fileName, dtype=np.uint8,\n",
        "              mode='r', shape=(trainingClassCount[classLabel],width,height,3))\n",
        "\n",
        "  imagesToLoad = round(trainingClassCount[classLabel] * customRatio[classLabel])\n",
        "  #genero delle immagini aggiuntive se custom load è > 1.0\n",
        "  imageToGenerate = max(0, imagesToLoad - trainingClassCount[classLabel])\n",
        "  indices = generateIndices(imageToGenerate, trainingClassCount[classLabel])\n",
        "  for index in indices:\n",
        "    trainingData[trainingCount] = generateRandomImage(generatorCustomLoad, trainingImages[index])/255.0\n",
        "    trainingLabels[trainingCount] = classLabel\n",
        "    trainingCount += 1\n",
        "  imagesToLoad = imagesToLoad - imageToGenerate\n",
        "  #numero di img aggiunte per la classe\n",
        "  indices = generateIndices(imagesToLoad, trainingClassCount[classLabel])\n",
        "  for index in indices:\n",
        "    trainingData[trainingCount] = trainingImages[index]/255.0\n",
        "    trainingLabels[trainingCount] = classLabel\n",
        "    trainingCount += 1\n",
        "  #carico le immagini di validation\n",
        "  validationImages = np.memmap(validationDir + fileName, dtype=np.uint8,\n",
        "              mode='r', shape=(validationClassCount[classLabel],width,height,3))\n",
        "\n",
        "  for img in validationImages:\n",
        "    validationData[validationCount] = img/255.0\n",
        "    validationLabels[validationCount] = classLabel\n",
        "    validationCount += 1\n",
        "\n",
        "  loadedClass += 1\n",
        "  if (loadedClass) % 5 == 0 or loadedClass == len(files):\n",
        "    print('Caricamento file ', loadedClass, 'su', len(files))\n",
        "\n",
        "# fine for caricamento classi\n",
        "# per ogni immagine genero delle nuove immagini con trasformazioni casuali\n",
        "if useDataAugmentation == True:\n",
        "  print(\"Genero le immagini aggiuntive\")\n",
        "  resetSeed(10)\n",
        "  for imageIndex in range(0, trainingCount):\n",
        "    for augIndex in range(0, NUMBER_OF_TRAINING_AUG):\n",
        "      trainingData[trainingCount] = generateRandomImage(generatorTraining, trainingData[imageIndex])/255.0\n",
        "      trainingLabels[trainingCount] = trainingLabels[imageIndex]\n",
        "      trainingCount += 1\n",
        "\n",
        "  resetSeed(-50)\n",
        "  for imageIndex in range(0, validationCount):\n",
        "    for augIndex in range(0, NUMBER_OF_VALIDATION_AUG):\n",
        "      validationData[validationCount] = generateRandomImage(generatorValidation, validationData[imageIndex])/255.0\n",
        "      validationLabels[validationCount] = validationLabels[imageIndex]\n",
        "      validationCount += 1\n",
        "  \n",
        "if trainingPadEnd == True:\n",
        "  for index in range(1, trainingToPad + 1):\n",
        "    trainingData[-index] = trainingData[len(trainingData) - (trainingToPad + 1)]\n",
        "    trainingLabels[-index] = trainingLabels[len(trainingLabels) - (trainingToPad + 1)]\n",
        "\n",
        "if validationPadEnd == True:\n",
        "  for index in range(1, validationToPad + 1):\n",
        "    validationData[-index] = validationData[len(validationData) - (validationToPad + 1)]\n",
        "    validationLabels[-index] = validationLabels[len(validationLabels) - (validationToPad + 1)]\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "print('Training', trainingData.shape) \n",
        "print('Validation', validationData.shape)\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.hist(trainingLabels, bins=43)\n",
        "plt.xticks(np.arange(0, 42, 1))\n",
        "plt.xlim(left=0, right=42)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urCz6X5MGZ7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parametri training\n",
        "trInfo = TrainingInfo.getDefaultTPU(trainingData,\n",
        "                                    trainingLabels,\n",
        "                                    validationData,\n",
        "                                    validationLabels)\n",
        "\n",
        "trInfo.setParameters(\n",
        "    learningRateList = [1e-2,1e-3,1e-4,1e-5,1e-6,1e-7],\n",
        "    fineTuningIterations = 0,\n",
        "    mainEpochs = 50, \n",
        "    metrics = ['sparse_categorical_accuracy']\n",
        ")\n",
        "\n",
        "models = Models()\n",
        "inputs = tf.keras.layers.Input(shape=(width, height, 3))\n",
        "###\n",
        "# DEFINIZIONE MODELLI\n",
        "###\n",
        "\n",
        "layer = buildDenseLayer(inputs, layers = 1, size = 64, regularizers = 0.01, flattenInput = True)\n",
        "outputs = buildDenseSoftmax(layer)\n",
        "model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
        "models.addModel('1D64', model, trInfo)\n",
        "\n",
        "layer = buildDenseLayer(inputs, layers = 2, size = 64, regularizers = 0.01, dropout = 0.3,flattenInput = True)\n",
        "outputs = buildDenseSoftmax(layer)\n",
        "model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
        "models.addModel('2D64', model, trInfo)\n",
        "\n",
        "\n",
        "layer = buildDenseLayer(inputs, layers = 1, size = 128, regularizers = 0.01,flattenInput = True)\n",
        "outputs = buildDenseSoftmax(layer)\n",
        "model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
        "models.addModel('1D128', model, trInfo)\n",
        "\n",
        "layer = buildDenseLayer(inputs, layers = 2, size = 128, regularizers = 0.01, dropout = 0.3,flattenInput = True)\n",
        "outputs = buildDenseSoftmax(layer)\n",
        "model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
        "models.addModel('2D128', model, trInfo)\n",
        "\n",
        "layer = buildConvLayer(inputs, layers = 1, size = 64, kernelSize = 3, flatten = True)\n",
        "outputs = buildDenseSoftmax(layer)\n",
        "model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
        "models.addModel('1C64_3', model, trInfo)\n",
        "\n",
        "\n",
        "layer = buildConvLayer(inputs, layers = 2, size = 64, kernelSize = 3, flatten = True)\n",
        "outputs = buildDenseSoftmax(layer)\n",
        "model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
        "models.addModel('2C64_3', model, trInfo)\n",
        "\n",
        "layer = buildConvLayer(inputs, layers = 2, size = 64, kernelSize = 3, flatten = True)\n",
        "layer =  buildDenseLayer(layer, layers = 1, size = 64, regularizers = 0.01, flattenInput = False)\n",
        "outputs = buildDenseSoftmax(layer)\n",
        "model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
        "models.addModel('2C64_3_1D64', model, trInfo)\n",
        "\n",
        "layer = buildConvBlock(inputs, layers = 2, size = 64, kernelSize = 3, poolSize = 2, poolStrides = 2, flatten = False)\n",
        "layer =  buildDenseLayer(layer, layers = 1, size = 64, regularizers = 0.01, flattenInput = True)\n",
        "outputs = buildDenseSoftmax(layer)\n",
        "model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
        "models.addModel('2C64_3_1MAX2_2_1D64', model, trInfo)\n",
        "\n",
        "\n",
        "layer = buildConvBlock(inputs, layers = 3, size = 64, kernelSize = 3, poolSize = 2, poolStrides = 2, flatten = False)\n",
        "layer = buildInceptionBlock(layer)\n",
        "outputs = buildGlobalSoftmax(layer)\n",
        "model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
        "models.addModel('3C64_3_1I_G', model, trInfo)\n",
        "\n",
        "\n",
        "layer = buildConvBlock(inputs, layers = 3, size = 64, kernelSize = 3, poolSize = 2, poolStrides = 2, flatten = False)\n",
        "layer = buildResNetBlock(layer, 128, 128, 512)\n",
        "outputs = buildGlobalSoftmax(layer)\n",
        "model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
        "models.addModel('3C64_3_1R_G', model, trInfo)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y_-ugGJM7U0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Training { vertical-output: true, display-mode: \"form\" }\n",
        "cleanLastSession = False #@param {type:\"boolean\"}\n",
        "if 'lastSession' in locals():\n",
        "  if cleanLastSession == True:\n",
        "    for trainedModel in lastSession:\n",
        "      print(\"Rimuovo il training del modello\", trainedModel)\n",
        "      shutil.rmtree(trainedModel)\n",
        "    lastSession = []\n",
        "else:\n",
        "  lastSession = []\n",
        "#@markdown ---\n",
        "verboseTraining = 1 #@param {type:\"integer\"}\n",
        "#@markdown ---\n",
        "mainTrainingEarlyStoppingDelta = 0.1 #@param {type:\"number\"}\n",
        "mainTrainingEarlyStoppinPatience = 10 #@param {type:\"integer\"}\n",
        "fineTuningEarlyStoppingDelta = 0.005 #@param {type:\"number\"}\n",
        "fineTuningEarlyStoppinPatience = 7 #@param {type:\"integer\"}\n",
        "#@markdown ---\n",
        "\n",
        "trainingPath = rootPath + 'trainingPhase1/'\n",
        "\n",
        "###\n",
        "# TRAINING\n",
        "###\n",
        "\n",
        "for modelName in models.getModelsName():\n",
        "  localtime = time.localtime(time.time())\n",
        "  model = models.getModel(modelName)\n",
        "  trainingInfo = models.getModelTrainingInfo(modelName)\n",
        "  lossFunction = trainingInfo.getLossFunction()\n",
        "  metrics = trainingInfo.getMetrics()\n",
        "  resumeTraining = trainingInfo.getResumeTraining()\n",
        "  #Prepare la directory principale training/modelNameAndtime/\n",
        "  skipModelDefinition = False\n",
        "  learningListToResume = []\n",
        "  if resumeTraining != None:\n",
        "    modelPath = trainingPath + modelName + \"_\" + resumeTraining[0] + \"/\"\n",
        "    skipModelDefinition = True\n",
        "    learningListToResume = [info[0] for info in resumeTraining[1]]\n",
        "  else:\n",
        "    modelPath = trainingPath + modelName + \"_\" + buildTimestampName(localtime) + \"/\"\n",
        "  \n",
        "  try:\n",
        "    lastSession.append(modelPath)\n",
        "    tensorboardPath = modelPath + \"tensorboard/\"\n",
        "    if skipModelDefinition == False:\n",
        "      os.mkdir(modelPath)\n",
        "      with open(modelPath + 'model.json', 'w') as modelFile:\n",
        "        jsonModel = model.to_json()\n",
        "        modelFile.write(jsonModel)\n",
        "      with open(modelPath + \"trainingParameters.txt\",\"w\") as trainingLog:\n",
        "        trainingLog.write(\"AUG : \"+ str(useDataAugmentation)+\"\\n\")\n",
        "        trainingLog.write(\"TR_AUG : \" + str(NUMBER_OF_TRAINING_AUG)+\"\\n\")\n",
        "        trainingLog.write(\"VAL_AUG : \" + str(NUMBER_OF_VALIDATION_AUG)+\"\\n\")\n",
        "        trainingLog.write(\"Custom_load : \" + str(useCustomLoad)+\"\\n\")\n",
        "        if useCustomLoad == True:\n",
        "          trainingLog.write(\"Custom_ratio : \" + str(customRatio) + \"\\n\")\n",
        "        freezeLayer = trainingInfo.getFreezeLayer()\n",
        "        trainingLog.write(\"freezeLayer : \" + str(freezeLayer)+ \"\\n\")\n",
        "        if freezeLayer == True:\n",
        "          trainingLog.write(\"freezeFrom : \" + str(trainingInfo.getFreezeFrom())+ \"\\n\")\n",
        "          trainingLog.write(\"freezeIndex : \" + str(trainingInfo.getLayersToFreeze())+ \"\\n\")\n",
        "    else:\n",
        "      with open(modelPath + \"resumedTrainingParameters.txt\",\"w\") as trainingLog:\n",
        "        trainingLog.write(\"AUG : \"+ str(useDataAugmentation)+\"\\n\")\n",
        "        trainingLog.write(\"TR_AUG : \" + str(NUMBER_OF_TRAINING_AUG)+\"\\n\")\n",
        "        trainingLog.write(\"VAL_AUG : \" + str(NUMBER_OF_VALIDATION_AUG)+\"\\n\")\n",
        "        trainingLog.write(\"Custom_load : \" + str(useCustomLoad)+\"\\n\")\n",
        "        if useCustomLoad == True:\n",
        "          trainingLog.write(\"Custom_ratio : \" + str(customRatio) + \"\\n\")\n",
        "        freezeLayer = trainingInfo.getFreezeLayer()\n",
        "        trainingLog.write(\"freezeLayer : \" + str(freezeLayer)+ \"\\n\")\n",
        "        if freezeLayer == True:\n",
        "          trainingLog.write(\"freezeFrom : \" + str(trainingInfo.getFreezeFrom())+ \"\\n\")\n",
        "          trainingLog.write(\"freezeIndex : \" + str(trainingInfo.getLayersToFreeze())+ \"\\n\")\n",
        "    for learningRate in trainingInfo.getLearningRateList():\n",
        "      if useTPU == True:\n",
        "        # se non si resetta la prima iterazione impiega sempre più tempo\n",
        "        tf.reset_default_graph() \n",
        "      print(Style.BRIGHT, \"\\n\" + modelName + \"] : main training con lr\" + str(learningRate), Style.RESET_ALL)\n",
        "      learningPath = modelPath + str(learningRate) + \"/\" #training/modelNameAndTime/learningRate/\n",
        "      mainTrainingPath = learningPath + \"Main/\" #training/modelNameAndTime/learningRate/Main/\n",
        "      weightsPath = mainTrainingPath + \"weights/\" #training/modelNameAndTime/learningRate/Main/weights/\n",
        "        \n",
        "      if learningRate not in learningListToResume:\n",
        "        # Preparo le direcotry \n",
        "        plotsPath = mainTrainingPath + \"plots/\" #training/modelNameAndTime/learningRate/Main/plots/\n",
        "        historyPath = mainTrainingPath + \"history/\" #training/modelNameAndTime/learningRate/Main/history/\n",
        "        makeDirs([learningPath, mainTrainingPath, weightsPath, plotsPath, historyPath])\n",
        "        # main training\n",
        "        tensorboardIterationPath = tensorboardPath + \"MainTraining_\" + str(learningRate) + \"/\"\n",
        "        checkpointPath = '/tmp/epoch_{epoch:02d}_valLoss_{val_loss:.4f}.hdf5'\n",
        "        #checkpointPath = weightsPath + \"epoch_{epoch:02d}_valLoss_{val_loss:.4f}.hdf5\"\n",
        "        callbacks = [\n",
        "            tensorboardCallback(logDir = tensorboardIterationPath,saveGraph = True, saveImages = True),\n",
        "            earlyStoppingCallback(mainTrainingEarlyStoppingDelta , mainTrainingEarlyStoppinPatience ),\n",
        "            checkpointCallback(fileName = checkpointPath, bestOnly = True, weightsOnly = True)\n",
        "        ]\n",
        "        weights = trainingInfo.getWeights()\n",
        "        weightsFromFile = trainingInfo.getWeightsFromFile()\n",
        "        \n",
        "        history, weights = trainModel(\n",
        "            model, learningRate,\n",
        "            trainingInfo.getTrainingData(), trainingInfo.getTrainingLabels(),\n",
        "            trainingInfo.getValidationData(), trainingInfo.getValidationLabels(),\n",
        "            trainingInfo.getMainEpochs(), trainingInfo.getShuffle(), trainingInfo.getBatchSize(),\n",
        "            lossFunction, metrics,\n",
        "            callbacks = callbacks, weights = weights,weightsFromFile = weightsFromFile, verbose = verboseTraining)\n",
        "        cleanCheckpointCallbackWeights('/tmp/')\n",
        "        lastWeights = list(filter(lambda fn: fn.startswith('epoch') and fn.endswith('.hdf5'),os.listdir('/tmp/')))[0]\n",
        "        shutil.move('/tmp/' + lastWeights, weightsPath + lastWeights)\n",
        "        #cleanCheckpointCallbackWeights(weightsPath)\n",
        "        saveLossPlot(plotsPath + \"validationLoss.png\", \"Validation\",[(history.history['val_loss'], learningRate)])\n",
        "        saveLossPlot(plotsPath + \"traininLoss.png\", \"Training\",[(history.history['loss'], learningRate)])\n",
        "        if 'val_sparse_categorical_accuracy' in history.history:\n",
        "          saveAccuracyPlot(plotsPath + \"validationAccuracy.png\", \"Validation\",[(history.history['val_sparse_categorical_accuracy'],learningRate)])\n",
        "        if 'sparse_categorical_accuracy' in history.history:\n",
        "          saveAccuracyPlot(plotsPath + \"trainingAccuracy.png\", \"Training\",[(history.history['sparse_categorical_accuracy'],learningRate)])\n",
        "        with open(historyPath + \"history.history\",\"wb\") as pickleOut:\n",
        "          pickle.dump(history.history, pickleOut)\n",
        "      else:\n",
        "        weights = retrieveWeights(model, weightsPath) \n",
        "      # fine-tuning\n",
        "      if trainingInfo.getFineTuningIterations() > 0:\n",
        "        fineTuningPath = learningPath + \"FineTuning/\" #training/modelNameAndTime/learningRate/FineTuning/\n",
        "        makeDirs([fineTuningPath])\n",
        "        if learningRate not in learningListToResume:\n",
        "          learningRateUpdater = trainingInfo.getLearningRateUpdater()\n",
        "        else:\n",
        "          learningRateUpdater = list(filter(lambda lrAndUpdater:lrAndUpdater[0] == learningRate,resumeTraining[1]))[0][1]\n",
        "        for iteration in range(trainingInfo.getFineTuningIterations()):\n",
        "          if useTPU == True:\n",
        "            # se non si resetta la prima iterazione impiega sempre più tempo\n",
        "            tf.reset_default_graph()\n",
        "          fineTuningLR = learningRateUpdater(learningRate, iteration + 1)\n",
        "          print(Style.BRIGHT, \"\\n\" + modelName + \"] : fine tuning con lr\" + str(fineTuningLR) + \"partendo da\" + str(learningRate), Style.RESET_ALL)\n",
        "          #preparo le directory\n",
        "          fineTuningLRPath = fineTuningPath + str(fineTuningLR) + \"/\" #training/modelNameAndTime/learningRate/FineTuning/LearningRate/\n",
        "          weightsPath = fineTuningLRPath + \"weights/\" #training/modelNameAndTime/learningRate/FineTuning/LearningRate/weights/\n",
        "          plotsPath = fineTuningLRPath + \"plots/\" #training/modelNameAndTime/learningRate/FineTuning/LearningRate/plots/\n",
        "          historyPath = fineTuningLRPath + \"history/\" #training/modelNameAndTime/learningRate/FineTuning/LearningRate/history/\n",
        "          makeDirs([fineTuningLRPath, weightsPath, plotsPath, historyPath])\n",
        "          #freezo i layer se necessario\n",
        "          if trainingInfo.getFreezeLayer() == True and trainingInfo.getFreezeFrom() - 1 >= iteration and trainingInfo.getLayersToFreeze >= 0:\n",
        "            for layerIndex in range(trainingInfo.getLayersToFreeze + 1):\n",
        "              model.layers[layerIndex].trainable = False\n",
        "          #faccio il training\n",
        "          tensorboardIterationPath = tensorboardPath + \"FineTuning_\" + str(learningRate) +  \"_\" + str(fineTuningLR) + \"/\"\n",
        "          checkpointPath = '/tmp/epoch_{epoch:02d}_valLoss_{val_loss:.4f}.hdf5'\n",
        "          #checkpointPath = weightsPath + \"epoch_{epoch:02d}_valLoss_{val_loss:.4f}.hdf5\"\n",
        "          callbacks = [\n",
        "              tensorboardCallback(logDir = tensorboardIterationPath,saveGraph = True, saveImages = True),\n",
        "              earlyStoppingCallback(fineTuningEarlyStoppingDelta , fineTuningEarlyStoppinPatience ),\n",
        "              checkpointCallback(fileName = checkpointPath, bestOnly = True, weightsOnly = True)\n",
        "          ]\n",
        "          history, weights = trainModel(\n",
        "              model, fineTuningLR,\n",
        "              trainingInfo.getTrainingData(), trainingInfo.getTrainingLabels(),\n",
        "              trainingInfo.getValidationData(), trainingInfo.getValidationLabels(),\n",
        "              trainingInfo.getFineTuningEpochs(), trainingInfo.getShuffle(), trainingInfo.getBatchSize(),\n",
        "              lossFunction, metrics,\n",
        "              callbacks = callbacks, weights = weights, verbose = verboseTraining)\n",
        "          cleanCheckpointCallbackWeights('/tmp/')\n",
        "          lastWeights = list(filter(lambda fn: fn.startswith('epoch') and fn.endswith('.hdf5'),os.listdir('/tmp/')))[0]\n",
        "          shutil.move('/tmp/' + lastWeights, weightsPath + lastWeights)\n",
        "          #cleanCheckpointCallbackWeights(weightsPath)\n",
        "          saveLossPlot(plotsPath + \"validationLoss.png\", \"Validation\",[(history.history['val_loss'], learningRate)])\n",
        "          saveLossPlot(plotsPath + \"traininLoss.png\", \"Training\",[(history.history['loss'], learningRate)])\n",
        "          if 'val_sparse_categorical_accuracy' in history.history:\n",
        "            saveAccuracyPlot(plotsPath + \"validationAccuracy.png\", \"Validation\",[(history.history['val_sparse_categorical_accuracy'],learningRate)])\n",
        "          if 'sparse_categorical_accuracy' in history.history:\n",
        "            saveAccuracyPlot(plotsPath + \"trainingAccuracy.png\", \"Training\",[(history.history['sparse_categorical_accuracy'],learningRate)])\n",
        "          with open(historyPath + \"history.history\",\"wb\") as pickleOut:\n",
        "            pickle.dump(history.history, pickleOut)\n",
        "    # fine ciclo sui learnin rate\n",
        "  except Exception as e:\n",
        "    print(\"Errore durante il training.\\n\")\n",
        "    lastSession.pop()\n",
        "    traceback.print_exc()\n",
        "    if 'resumeTraining' in locals():\n",
        "      if resumeTraining != None:\n",
        "        print(\"Cleaning\", modelPath)\n",
        "        shutil.rmtree(modelPath)\n",
        "    else:\n",
        "      print(\"Cleaning\", modelPath)\n",
        "      shutil.rmtree(modelPath)\n",
        "    \n",
        "print(\"Fine training\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}